{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "from torchvision import datasets, transforms\n",
    "from torch import nn, optim\n",
    "import cv2\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import shutil\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5750, 2)\n",
      "(5750, 80, 80, 3)\n",
      "[1 1 0 ... 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "tr_d = 'dogs-vs-cats/train/'   #minitrain of 20 images\n",
    "te_d = 'dogs-vs-cats/randtest/'\n",
    "\n",
    "onlyfiles = [f for f in listdir('dogs-vs-cats/train/') if isfile(join('dogs-vs-cats/train/', f))]\n",
    "onlytestfiles = [f for f in listdir('dogs-vs-cats/randtest/') if isfile(join('dogs-vs-cats/randtest/', f))]\n",
    "\n",
    "trainset = []\n",
    "for i in range(len(onlyfiles)):\n",
    "    if i%4 == 0: \n",
    "        im = cv2.imread(tr_d + onlyfiles[i], cv2.IMREAD_COLOR)\n",
    "        im = cv2.resize(im, (80, 80))\n",
    "        #image = cv2.imread(\"lenacolor512.tiff\", cv2.IMREAD_COLOR)  # uint8 image\n",
    "        im = cv2.normalize(im, None, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
    "        #print(tr_d + 'cat.' + str(i) + '.jpg')\n",
    "        #print(im)\n",
    "        flag = 0\n",
    "        if onlyfiles[i][:3] == 'dog':\n",
    "            flag = 1\n",
    "            #print(i)\n",
    "        trainset.append([im, flag])\n",
    "# for i in range(20):\n",
    "#     im = cv2.imread(tr_d + 'dog.' + str(i) + '.jpg', cv2.IMREAD_COLOR)\n",
    "#     im = cv2.resize(im, (50, 50))\n",
    "#     im = cv2.normalize(im, None, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
    "#    \n",
    "#     trainset.append([im, 1])\n",
    "#print(trainset[0][0])\n",
    "trainset = np.array(trainset)\n",
    "#print(trainset[0][0])\n",
    "np.random.shuffle(trainset)\n",
    "print(trainset.shape)\n",
    "\n",
    "images = []\n",
    "labels = []\n",
    "for i in range(trainset.shape[0]):\n",
    "    images.append(trainset[i][0])\n",
    "    labels.append(trainset[i][1])\n",
    "#print(labels)\n",
    "images = np.array(images)\n",
    "labels = np.array(labels)\n",
    "print(images.shape)\n",
    "print(labels)\n",
    "#im = cv2.imread(\"dogs-vs-cats/minitrain/cat.0.jpg\")\n",
    "\n",
    "#ADD TEST SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(57, 100, 80, 80, 3)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "b_im = []\n",
    "b_val = []\n",
    "for i in range(images.shape[0]//batch_size):\n",
    "    b_im.append(images[i*batch_size:(i+1)*batch_size])\n",
    "    b_val.append(labels[i*batch_size:(i+1)*batch_size])\n",
    "\n",
    "b_im = np.array(b_im)\n",
    "b_val = np.array(b_val)\n",
    "\n",
    "print(b_im.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=19200, out_features=1024, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=1024, out_features=512, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=512, out_features=2, bias=True)\n",
      "  (5): LogSoftmax()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "input_size = 80*80*3\n",
    "hidden_sizes = [1024, 512]\n",
    "output_size = 2\n",
    "\n",
    "model = nn.Sequential(nn.Linear(input_size, hidden_sizes[0]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[1], output_size),\n",
    "                      nn.LogSoftmax(dim=1))\n",
    "print(model)\n",
    "\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Training loss: 0.006774944129197494\n",
      "Epoch 1 - Training loss: 0.006539215989734816\n",
      "Epoch 2 - Training loss: 0.006398838385291722\n",
      "Epoch 3 - Training loss: 0.006287819561750992\n",
      "Epoch 4 - Training loss: 0.006197369326715884\n",
      "Epoch 5 - Training loss: 0.006138625580331554\n",
      "Epoch 6 - Training loss: 0.006091786550438923\n",
      "Epoch 7 - Training loss: 0.006027442133944967\n",
      "Epoch 8 - Training loss: 0.006065825514171434\n",
      "Epoch 9 - Training loss: 0.0060285511016845705\n",
      "Epoch 10 - Training loss: 0.0058999953269958496\n",
      "Epoch 11 - Training loss: 0.005964245376379593\n",
      "Epoch 12 - Training loss: 0.005886223875957986\n",
      "Epoch 13 - Training loss: 0.005813477635383606\n",
      "Epoch 14 - Training loss: 0.005981547842855039\n",
      "Epoch 15 - Training loss: 0.005877230042996614\n",
      "Epoch 16 - Training loss: 0.005938295986341394\n",
      "Epoch 17 - Training loss: 0.005738435791886371\n",
      "Epoch 18 - Training loss: 0.005904546758402949\n",
      "Epoch 19 - Training loss: 0.005689750764680946\n",
      "Epoch 20 - Training loss: 0.005507812474084937\n",
      "Epoch 21 - Training loss: 0.005723059519477512\n",
      "Epoch 22 - Training loss: 0.0053656539606011435\n",
      "Epoch 23 - Training loss: 0.0054035385328790415\n",
      "Epoch 24 - Training loss: 0.005383067571598551\n",
      "\n",
      "Training Time (in minutes) = 12.350561165809632\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9) #was 0.004, 0.9\n",
    "time0 = time()\n",
    "epochs = 25\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for images, labels in zip(b_im, b_val):\n",
    "        images = torch.FloatTensor(images)\n",
    "        images = images.view(images.shape[0], -1)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(images)\n",
    "        \n",
    "        labels = torch.LongTensor(labels)\n",
    "        loss = criterion(output, labels)\n",
    "        \n",
    "        #This is where the model learns by backpropagating\n",
    "        loss.backward()\n",
    "        \n",
    "        #And optimizes its weights here\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    else:\n",
    "        print(\"Epoch {} - Training loss: {}\".format(e, running_loss/trainset.shape[0]))\n",
    "print(\"\\nTraining Time (in minutes) =\",(time()-time0)/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 2)\n",
      "(2000, 80, 80, 3)\n"
     ]
    }
   ],
   "source": [
    "testset = []\n",
    "for i in range(len(onlytestfiles)):\n",
    "    im = cv2.imread(te_d + onlytestfiles[i], cv2.IMREAD_COLOR)\n",
    "    im = cv2.resize(im, (80, 80))\n",
    "    #image = cv2.imread(\"lenacolor512.tiff\", cv2.IMREAD_COLOR)  # uint8 image\n",
    "    im = cv2.normalize(im, None, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
    "    #print(tr_d + 'cat.' + str(i) + '.jpg')\n",
    "    #print(im)\n",
    "    flag = 0\n",
    "    if onlytestfiles[i][:3] == 'dog':\n",
    "        flag = 1\n",
    "    testset.append([im, flag])\n",
    "# for i in range(20):\n",
    "#     im = cv2.imread(tr_d + 'dog.' + str(i) + '.jpg', cv2.IMREAD_COLOR)\n",
    "#     im = cv2.resize(im, (50, 50))\n",
    "#     im = cv2.normalize(im, None, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
    "#    \n",
    "#     trainset.append([im, 1])\n",
    "#print(trainset[0][0])\n",
    "testset = np.array(testset)\n",
    "#print(trainset[0][0])\n",
    "np.random.shuffle(testset)\n",
    "print(testset.shape)\n",
    "\n",
    "images = []\n",
    "labels = []\n",
    "for i in range(testset.shape[0]):\n",
    "    images.append(testset[i][0])\n",
    "    labels.append(testset[i][1])\n",
    "#print(labels)\n",
    "images = np.array(images)\n",
    "labels = np.array(labels)\n",
    "print(images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 100, 80, 80, 3)\n"
     ]
    }
   ],
   "source": [
    "t_b_im = []\n",
    "t_b_val = []\n",
    "for i in range(images.shape[0]//batch_size):\n",
    "    t_b_im.append(images[i*batch_size:(i+1)*batch_size])\n",
    "    t_b_val.append(labels[i*batch_size:(i+1)*batch_size])\n",
    "\n",
    "t_b_im = np.array(t_b_im)\n",
    "t_b_val = np.array(t_b_val)\n",
    "\n",
    "print(t_b_im.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_count, all_count = 0, 0\n",
    "for images, labels in zip(t_b_im, t_b_val):\n",
    "        images = torch.FloatTensor(images)\n",
    "        #print(t_b_im.shape)\n",
    "        images = images.view(images.shape[0], -1)\n",
    "        #print(images.shape)\n",
    "        # Training pass\n",
    "        with torch.no_grad():\n",
    "            logps = model(images)\n",
    "        #print(logps.shape, logps[0])\n",
    "        ps = torch.exp(logps)\n",
    "        #print(ps.shape)\n",
    "        for i in range(batch_size):\n",
    "            prob_ans = ps[i]\n",
    "            prob_ans = list(prob_ans)\n",
    "            pred_label = prob_ans.index(max(prob_ans))\n",
    "            #pred_label = ps.index(max(ps))\n",
    "            true_label = labels[i]\n",
    "            if(true_label == pred_label):\n",
    "              correct_count += 1\n",
    "            all_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n",
      "0.613\n"
     ]
    }
   ],
   "source": [
    "print(all_count)\n",
    "print(correct_count/all_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
