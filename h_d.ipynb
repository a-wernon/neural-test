{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "from torchvision import datasets, transforms\n",
    "from torch import nn, optim\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.0, 1, 1, 1, 0.0, 1, 1, 0.0, 1, 1, 0.0, 1, 0.0, 0.0, 1, 1, 0.0, 0.0, 0.0, 1, 1, 0.0, 1, 0.0, 0.0, 1, 0.0, 0.0, 0.0, 0.0, 0.0, 1, 1, 0.0, 0.0, 0.0, 1, 1, 1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1, 0.0, 0.0, 0.0, 1, 1, 0.0, 1, 0.0, 0.0, 0.0, 1, 0.0, 1, 0.0, 1, 1, 0.0, 1, 1, 1, 0.0, 1, 1, 0.0, 0.0, 1, 1, 0.0, 0.0, 1, 0.0, 0.0, 0.0, 1, 1, 1, 0.0, 0.0, 0.0, 1, 0.0, 0.0, 1, 0.0, 0.0, 0.0, 1, 0.0, 0.0, 1, 1, 0.0, 0.0, 1, 1, 0.0, 1, 1, 1, 0.0, 1, 0.0, 1, 0.0, 0.0, 0.0, 0.0, 1, 0.0, 1, 0.0, 1, 1, 1, 1, 1, 0.0, 1, 1, 1, 0.0, 1, 0.0, 1, 0.0, 1, 0.0, 0.0, 1, 0.0, 0.0, 0.0, 0.0, 1, 0.0, 1, 0.0, 0.0, 1, 0.0, 1, 1, 1, 1, 0.0, 1, 1, 1, 1, 1, 1, 0.0, 1, 1, 1, 0.0, 0.0, 0.0, 1, 0.0, 0.0, 0.0, 1, 1, 0.0, 0.0, 1, 0.0, 1, 1, 1, 0.0, 0.0, 0.0, 1, 1, 1, 1, 0.0, 1, 0.0, 1, 1, 1, 0.0, 0.0, 0.0, 0.0, 1, 1, 0.0, 1, 0.0, 1, 0.0, 0.0, 1, 1, 0.0, 0.0, 0.0, 0.0, 1, 1, 1, 0.0, 1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1, 1, 0.0, 0.0, 1, 0.0, 1, 1, 0.0, 1, 0.0, 1, 0.0, 0.0, 1, 1, 1, 1, 1, 0.0, 1, 1, 1, 1, 1, 0.0, 0.0, 1, 0.0, 0.0, 1, 0.0, 1, 1, 0.0, 1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1, 0.0, 1, 0.0, 1, 0.0, 0.0, 0.0, 1, 0.0, 0.0, 0.0, 0.0, 0.0, 1, 0.0, 0.0, 0.0, 0.0, 1, 0.0, 0.0, 1, 1, 0.0, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,)),])\n",
    "a = open('processed.cleveland.data', 'r')\n",
    "c = a.readlines()\n",
    "random.shuffle(c) #shuffle data before using\n",
    "n_val = []\n",
    "n_keys = []\n",
    "d = []\n",
    "for elem in c:\n",
    "    elem = elem[:-1]\n",
    "    elem = list(map(float,elem.split(',')))\n",
    "    n_val.append(elem[:-1])\n",
    "    n_keys.append(elem[-1]) #mb ad\n",
    "rot = [[] for i in range(len(n_val[0]))]\n",
    "mean = []\n",
    "std = []\n",
    "for i in range(len(n_val)):\n",
    "    for j in range(len(n_val[i])):\n",
    "        rot[j].append(n_val[i][j])\n",
    "#print(len(rot[0]))\n",
    "for elem in rot:\n",
    "    #print(elem)\n",
    "    new_elem = np.array(elem)\n",
    "    #print(np.std(new_elem))\n",
    "    #print(new_elem.shape)\n",
    "    std.append(np.std(new_elem))\n",
    "    mean.append(np.mean(new_elem))\n",
    "    #elem = [int(elem[i]*10) for i in range(len(elem))] \n",
    "for i in range(len(rot)):\n",
    "    rot[i] = [(rot[i][j] - mean[i])/std[i] for j in range(len(elem))]\n",
    "#print(rot[0])\n",
    "rot = np.array(rot)\n",
    "n_val = rot.T     #all of the previous 20 lines normalises our array\n",
    "\n",
    "\n",
    "#true-false\n",
    "for i in range(len(n_keys)):\n",
    "    if n_keys[i] > 0:\n",
    "        n_keys[i] = 1\n",
    "print(n_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(225, 13)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#73 - test size\n",
    "#230 - train size\n",
    "t = 230\n",
    "te = 73\n",
    "a = random.sample(range(303), t)\n",
    "val, keys, te_val, te_keys = [],[],[],[]\n",
    "for i in range(len(n_val)):\n",
    "    if i in a:\n",
    "        val.append(n_val[i])\n",
    "        keys.append(n_keys[i])\n",
    "    else:\n",
    "        te_val.append(n_val[i])                       #train-test \n",
    "        te_keys.append(n_keys[i])\n",
    "\n",
    "val = np.array(val)\n",
    "keys = np.array(keys)\n",
    "#val = val.astype(float)\n",
    "#keys = keys.astype(float)\n",
    "\n",
    "te_val = np.array(te_val)\n",
    "te_keys = np.array(te_keys)\n",
    "#keys.shape\n",
    "val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=13, out_features=128, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=128, out_features=2, bias=True)\n",
      "  (5): LogSoftmax()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "input_size = 13\n",
    "hidden_sizes = [128, 128]\n",
    "output_size = 2\n",
    "\n",
    "model = nn.Sequential(nn.Linear(input_size, hidden_sizes[0]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[1], output_size),\n",
    "                      nn.LogSoftmax(dim=1))\n",
    "print(model) #model with 3 layers, first 2 with relu, last with logistic curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss() #MSELoss() \n",
    "#images, labels = next(iter(trainloader))\n",
    "#images = images.view(images.shape[0], -1)\n",
    "\n",
    "#logps = model(images) #log probabilities\n",
    "#loss = criterion(logps, labels) #calculate the NLL loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Training loss: 0.47246257845105394\n",
      "Epoch 1 - Training loss: 0.36159670289838686\n",
      "Epoch 2 - Training loss: 0.3086192248691481\n",
      "Epoch 3 - Training loss: 0.2644419637817878\n",
      "Epoch 4 - Training loss: 0.22310542189092203\n",
      "Epoch 5 - Training loss: 0.18992356225372015\n",
      "Epoch 6 - Training loss: 0.1517888315872963\n",
      "Epoch 7 - Training loss: 0.15815258738369375\n",
      "Epoch 8 - Training loss: 0.10645577385676303\n",
      "Epoch 9 - Training loss: 0.0811360431769501\n",
      "Epoch 10 - Training loss: 0.07258982644413815\n",
      "Epoch 11 - Training loss: 0.0596769250681537\n",
      "Epoch 12 - Training loss: 0.043876746637515146\n",
      "Epoch 13 - Training loss: 0.05491245090027721\n",
      "Epoch 14 - Training loss: 0.30292342605293004\n",
      "Epoch 15 - Training loss: 0.14503666615039384\n",
      "Epoch 16 - Training loss: 0.08022974246899375\n",
      "Epoch 17 - Training loss: 0.03043984462018039\n",
      "Epoch 18 - Training loss: 0.013861719567016262\n",
      "Epoch 19 - Training loss: 0.007235605643078568\n",
      "Epoch 20 - Training loss: 0.004534909595122308\n",
      "Epoch 21 - Training loss: 0.0034871170484640224\n",
      "Epoch 22 - Training loss: 0.0028078377595668435\n",
      "Epoch 23 - Training loss: 0.002358466245421514\n",
      "Epoch 24 - Training loss: 0.002025214111573285\n",
      "Epoch 25 - Training loss: 0.0017673346931339515\n",
      "Epoch 26 - Training loss: 0.0015688297919535267\n",
      "Epoch 27 - Training loss: 0.001411472062821133\n",
      "Epoch 28 - Training loss: 0.001277651058431483\n",
      "Epoch 29 - Training loss: 0.0011703186253364114\n",
      "\n",
      "Training Time (in minutes) = 0.16393086512883503\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.005, momentum=0.9)\n",
    "time0 = time()\n",
    "epochs = 30\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for i in range(len(val)):\n",
    "        # Training pass\n",
    "        optimizer.zero_grad()\n",
    "        temp_1 = [val[i]]\n",
    "        temp0 = np.array(temp_1)\n",
    "        temp = torch.from_numpy(temp0)\n",
    "        temp1 = temp.float()\n",
    "        #print(temp1)\n",
    "        output = model(temp1)    #DOES NOT WORK. MODEL RETURNS NAN TENSOR\n",
    "        #print(output)\n",
    "        k = [keys[i]]\n",
    "        k = np.array(k)\n",
    "        k = torch.from_numpy(k)\n",
    "        k = k.long()\n",
    "        #print(output.shape)\n",
    "        #print(k.shape)\n",
    "        #print('result', e, output)\n",
    "        #print(k)\n",
    "        loss = criterion(output, k)\n",
    "        #print(loss)\n",
    "        \n",
    "        #This is where the model learns by backpropagating\n",
    "        loss.backward()\n",
    "        \n",
    "        #And optimizes its weights here\n",
    "        optimizer.step()\n",
    "        \n",
    "        #print(loss.item())\n",
    "        running_loss += loss.item()\n",
    "    else:\n",
    "        print(\"Epoch {} - Training loss: {}\".format(e, running_loss/len(val)))\n",
    "\n",
    "print(\"\\nTraining Time (in minutes) =\",(time()-time0)/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number Of Images Tested = 72\n",
      "\n",
      "Model Accuracy = 0.7361111111111112\n"
     ]
    }
   ],
   "source": [
    "correct_count, all_count = 0, 0    \n",
    "norm_c, norm_true = 0,0\n",
    "dis_c, dis_true = 0,0\n",
    "\n",
    "for i in range(len(te_val)):\n",
    "    temp_1 = [te_val[i]]\n",
    "    temp0 = np.array(temp_1)\n",
    "    temp = torch.from_numpy(temp0)\n",
    "    temp1 = temp.float()\n",
    "    with torch.no_grad():\n",
    "        logps = model(temp1)\n",
    "    ps = torch.exp(logps)\n",
    "    probab = list(ps.numpy()[0])\n",
    "    pred_label = probab.index(max(probab))\n",
    "    true_label = te_keys[i]\n",
    "    if(true_label == pred_label):\n",
    "        correct_count += 1\n",
    "        if true_label == 0:\n",
    "            norm_true += 1\n",
    "        else:\n",
    "            dis_true += 1\n",
    "    if true_label == 0:\n",
    "        norm_c += 1\n",
    "    else:\n",
    "        dis_c += 1        \n",
    "    all_count += 1\n",
    "\n",
    "print(\"Number Of Images Tested =\", all_count)\n",
    "print(\"\\nModel Accuracy =\", (correct_count/all_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number Of norm = 38\n",
      "\n",
      "Model Accuracy norm= 0.8157894736842105\n",
      "Number Of dis = 34\n",
      "\n",
      "Model Accuracy dis= 0.6470588235294118\n"
     ]
    }
   ],
   "source": [
    "#true-false 80 percent\n",
    "print(\"Number Of norm =\", norm_c)\n",
    "print(\"\\nModel Accuracy norm=\", (norm_true/norm_c))\n",
    "print(\"Number Of dis =\", dis_c)\n",
    "print(\"\\nModel Accuracy dis=\", (dis_true/dis_c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, './my_mnist_model.pt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
