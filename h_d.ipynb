{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "from torchvision import datasets, transforms\n",
    "from torch import nn, optim\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "207.0 90.0\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,)),])\n",
    "a = open('processed.cleveland.data', 'r')\n",
    "c = a.readlines()\n",
    "random.shuffle(c) #shuffle data before using\n",
    "all_val, all_keys = [], []\n",
    "for elem in c:\n",
    "    elem = elem[:-1]\n",
    "    elem = list(map(float,elem.split(',')))\n",
    "    all_val.append(elem[:-1])\n",
    "    all_keys.append(elem[-1])\n",
    "all_val = np.array(all_val).T\n",
    "\n",
    "std = []\n",
    "mean = []\n",
    "\n",
    "for elem in all_val:\n",
    "    #print(elem)\n",
    "    new_elem = np.array(elem)\n",
    "    #print(np.std(new_elem))\n",
    "    #print(new_elem.shape)\n",
    "    std.append(np.std(new_elem))\n",
    "    mean.append(np.mean(new_elem))\n",
    "    #elem = [int(elem[i]*10) for i in range(len(elem))] \n",
    "for i in range(len(all_val)):\n",
    "    all_val[i] = [(all_val[i][j] - mean[i])/std[i] for j in range(len(elem))]\n",
    "\n",
    "all_val = all_val.T\n",
    "\n",
    "d_val = []\n",
    "d_keys = []\n",
    "d = []\n",
    "t = (len(c) * 0.7)//1\n",
    "te = len(c) - t\n",
    "print(t, te)\n",
    "a = random.sample(range(int(t + te)), int(t))\n",
    "val, keys, te_val, te_keys = [],[],[],[]\n",
    "for i in range(len(all_val)):\n",
    "    if i in a:\n",
    "        val.append(all_val[i])\n",
    "        if all_keys[i] > 0:\n",
    "            keys.append(1)\n",
    "        else: \n",
    "            keys.append(0)\n",
    "        #n_keys.append(elem[-1])\n",
    "        if all_keys[i] > 0:\n",
    "            d_keys.append(all_keys[i] - 1)\n",
    "            d_val.append(all_val[i])\n",
    "    else:\n",
    "        te_val.append(all_val[i])\n",
    "        te_keys.append(all_keys[i])\n",
    "        \n",
    "    #now we want to normalise all values\n",
    "# rot = [[] for i in range(len(n_val[0]))]\n",
    "# mean = []\n",
    "# std = []\n",
    "# for i in range(len(n_val)):\n",
    "#     for j in range(len(n_val[i])):\n",
    "#         rot[j].append(n_val[i][j])\n",
    "# #print(len(rot[0]))\n",
    "\n",
    "# #print(rot[0])\n",
    "# rot = np.array(rot)\n",
    "# n_val = rot.T     #all of the previous 20 lines normalises our array\n",
    "\n",
    "\n",
    "# #true-false\n",
    "# for i in range(len(n_keys)):\n",
    "#     if n_keys[i] > 0:\n",
    "#         n_keys[i] = 1\n",
    "# print(n_keys)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'float' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-177-53c7d0735bfd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_val\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m0.7\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mte\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_val\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mte\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mte_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mte_keys\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'float' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "#73 - test size\n",
    "#230 - train size\n",
    "t = (len(n_val) * 0.7) %1\n",
    "te = len(n_val) - t\n",
    "a = random.sample(range(t + te), t)\n",
    "val, keys, te_val, te_keys = [],[],[],[]\n",
    "for i in range(len(n_val)):\n",
    "    if i in a:\n",
    "        val.append(n_val[i])\n",
    "        keys.append(n_keys[i])\n",
    "    else:\n",
    "        te_val.append(n_val[i])                       #train-test \n",
    "        te_keys.append(n_keys[i])\n",
    "\n",
    "val = np.array(val)\n",
    "keys = np.array(keys)\n",
    "#val = val.astype(float)\n",
    "#keys = keys.astype(float)\n",
    "\n",
    "te_val = np.array(te_val)\n",
    "te_keys = np.array(te_keys)\n",
    "#keys.shape\n",
    "val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #73 - test size\n",
    "# #230 - train size\n",
    "# d_t = (len(dis_val) * 0.7) %1\n",
    "# d_te = len(dis_val) - t\n",
    "# a = random.sample(range(d_t + d_te), d_t)\n",
    "# d_val, d_keys, d_te_val, d_te_keys = [],[],[],[]\n",
    "# for i in range(len(dis_val)):\n",
    "#     if i in a:\n",
    "#         d_val.append(dis_val[i])\n",
    "#         d_keys.append(dis_keys[i])\n",
    "#     else:\n",
    "#         d_te_val.append(dis_val[i])                       #train-test \n",
    "#         d_te_keys.append(dis_keys[i])\n",
    "\n",
    "# d_val = np.array(d_val)\n",
    "# d_keys = np.array(d_keys)\n",
    "# #val = val.astype(float)\n",
    "# #keys = keys.astype(float)\n",
    "\n",
    "# d_te_val = np.array(d_te_val)\n",
    "# d_te_keys = np.array(d_te_keys)\n",
    "# #keys.shape\n",
    "# val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=13, out_features=256, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=128, out_features=2, bias=True)\n",
      "  (5): LogSoftmax()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "input_size = 13\n",
    "hidden_sizes = [256, 128] #was128\n",
    "output_size = 2\n",
    "\n",
    "model = nn.Sequential(nn.Linear(input_size, hidden_sizes[0]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[1], output_size),\n",
    "                      nn.LogSoftmax(dim=1))\n",
    "print(model) #model with 3 layers, first 2 with relu, last with logistic curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=13, out_features=256, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=128, out_features=4, bias=True)\n",
      "  (5): LogSoftmax()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "input_size = 13\n",
    "hidden_sizes = [256, 128] #was128\n",
    "output_size = 4\n",
    "\n",
    "d_model = nn.Sequential(nn.Linear(input_size, hidden_sizes[0]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[1], output_size),\n",
    "                      nn.LogSoftmax(dim=1))\n",
    "print(d_model) #model with 3 layers, first 2 with relu, last with logistic curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss() #MSELoss() \n",
    "#images, labels = next(iter(trainloader))\n",
    "#images = images.view(images.shape[0], -1)\n",
    "\n",
    "#logps = model(images) #log probabilities\n",
    "#loss = criterion(logps, labels) #calculate the NLL loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Training loss: 0.47633558931096304\n",
      "Epoch 1 - Training loss: 0.38341724949542016\n",
      "Epoch 2 - Training loss: 0.34652491575957245\n",
      "Epoch 3 - Training loss: 0.31303071337481525\n",
      "Epoch 4 - Training loss: 0.27776940920326154\n",
      "Epoch 5 - Training loss: 0.2386239735093384\n",
      "Epoch 6 - Training loss: 0.1987337485960751\n",
      "Epoch 7 - Training loss: 0.14483016017170883\n",
      "Epoch 8 - Training loss: 0.25697512233852193\n",
      "Epoch 9 - Training loss: 0.22036494513445187\n",
      "Epoch 10 - Training loss: 0.16510987051958453\n",
      "Epoch 11 - Training loss: 0.07984425564911216\n",
      "Epoch 12 - Training loss: 0.05534398595696324\n",
      "Epoch 13 - Training loss: 0.033220136831430824\n",
      "Epoch 14 - Training loss: 0.023261321930241635\n",
      "Epoch 15 - Training loss: 0.017709181021697593\n",
      "Epoch 16 - Training loss: 0.014509133698282307\n",
      "Epoch 17 - Training loss: 0.012630663723932607\n",
      "Epoch 18 - Training loss: 0.010794019750365029\n",
      "Epoch 19 - Training loss: 0.009188434865143967\n",
      "Epoch 20 - Training loss: 0.007982969051285533\n",
      "Epoch 21 - Training loss: 0.006927832546096221\n",
      "Epoch 22 - Training loss: 0.006516359310284201\n",
      "Epoch 23 - Training loss: 0.005768685108684052\n",
      "Epoch 24 - Training loss: 0.005413201173314698\n",
      "Epoch 25 - Training loss: 0.004934720799122615\n",
      "Epoch 26 - Training loss: 0.004579830140418965\n",
      "Epoch 27 - Training loss: 0.004347085039279933\n",
      "Epoch 28 - Training loss: 0.004045067437949212\n",
      "Epoch 29 - Training loss: 0.003856171284910018\n",
      "\n",
      "Training Time (in minutes) = 0.14547927379608155\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.005, momentum=0.9)\n",
    "time0 = time()\n",
    "epochs = 30\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for i in range(len(val)):\n",
    "        # Training pass\n",
    "        optimizer.zero_grad()\n",
    "        temp_1 = [val[i]]\n",
    "        temp0 = np.array(temp_1)\n",
    "        temp = torch.from_numpy(temp0)\n",
    "        temp1 = temp.float()\n",
    "        #print(temp1)\n",
    "        output = model(temp1)    #DOES NOT WORK. MODEL RETURNS NAN TENSOR\n",
    "        #print(output)\n",
    "        k = [keys[i]]\n",
    "        k = np.array(k)\n",
    "        k = torch.from_numpy(k)\n",
    "        k = k.long()\n",
    "        #print(output.shape)\n",
    "        #print(k.shape)\n",
    "        #print('result', e, output)\n",
    "        #print(k)\n",
    "        loss = criterion(output, k)\n",
    "        #print(loss)\n",
    "        \n",
    "        #This is where the model learns by backpropagating\n",
    "        loss.backward()\n",
    "        \n",
    "        #And optimizes its weights here\n",
    "        optimizer.step()\n",
    "        \n",
    "        #print(loss.item())\n",
    "        running_loss += loss.item()\n",
    "    else:\n",
    "        print(\"Epoch {} - Training loss: {}\".format(e, running_loss/len(val)))\n",
    "\n",
    "print(\"\\nTraining Time (in minutes) =\",(time()-time0)/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Training loss: 0.6487609366863822\n",
      "Epoch 1 - Training loss: 0.6487609366863822\n",
      "Epoch 2 - Training loss: 0.6487609366863822\n",
      "Epoch 3 - Training loss: 0.6487609366863822\n",
      "Epoch 4 - Training loss: 0.6487609366863822\n",
      "Epoch 5 - Training loss: 0.6487609366863822\n",
      "Epoch 6 - Training loss: 0.6487609366863822\n",
      "Epoch 7 - Training loss: 0.6487609366863822\n",
      "Epoch 8 - Training loss: 0.6487609366863822\n",
      "Epoch 9 - Training loss: 0.6487609366863822\n",
      "Epoch 10 - Training loss: 0.6487609366863822\n",
      "Epoch 11 - Training loss: 0.6487609366863822\n",
      "Epoch 12 - Training loss: 0.6487609366863822\n",
      "Epoch 13 - Training loss: 0.6487609366863822\n",
      "Epoch 14 - Training loss: 0.6487609366863822\n",
      "Epoch 15 - Training loss: 0.6487609366863822\n",
      "Epoch 16 - Training loss: 0.6487609366863822\n",
      "Epoch 17 - Training loss: 0.6487609366863822\n",
      "Epoch 18 - Training loss: 0.6487609366863822\n",
      "Epoch 19 - Training loss: 0.6487609366863822\n",
      "Epoch 20 - Training loss: 0.6487609366863822\n",
      "Epoch 21 - Training loss: 0.6487609366863822\n",
      "Epoch 22 - Training loss: 0.6487609366863822\n",
      "Epoch 23 - Training loss: 0.6487609366863822\n",
      "Epoch 24 - Training loss: 0.6487609366863822\n",
      "Epoch 25 - Training loss: 0.6487609366863822\n",
      "Epoch 26 - Training loss: 0.6487609366863822\n",
      "Epoch 27 - Training loss: 0.6487609366863822\n",
      "Epoch 28 - Training loss: 0.6487609366863822\n",
      "Epoch 29 - Training loss: 0.6487609366863822\n",
      "\n",
      "Training Time (in minutes) = 0.14764612913131714\n"
     ]
    }
   ],
   "source": [
    "d_optimizer = optim.SGD(model.parameters(), lr=0.005, momentum=0.9)\n",
    "time0 = time()\n",
    "epochs = 30\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for i in range(len(d_val)):\n",
    "        # Training pass\n",
    "        optimizer.zero_grad()\n",
    "        temp_1 = [d_val[i]]\n",
    "        temp0 = np.array(temp_1)\n",
    "        temp = torch.from_numpy(temp0)\n",
    "        temp1 = temp.float()\n",
    "        #print(temp1)\n",
    "        output = d_model(temp1)    #DOES NOT WORK. MODEL RETURNS NAN TENSOR\n",
    "        #print(output)\n",
    "        k = [d_keys[i]]\n",
    "        k = np.array(k)\n",
    "        k = torch.from_numpy(k)\n",
    "        k = k.long()\n",
    "        #print(output.shape)\n",
    "        #print(k.shape)\n",
    "        #print('result', e, output)\n",
    "        #print(k)\n",
    "        #print(output)\n",
    "        #print(k)\n",
    "        loss = criterion(output, k)\n",
    "        #print(loss)\n",
    "        \n",
    "        #This is where the model learns by backpropagating\n",
    "        loss.backward()\n",
    "        \n",
    "        #And optimizes its weights here\n",
    "        optimizer.step()\n",
    "        \n",
    "        #print(loss.item())\n",
    "        running_loss += loss.item()\n",
    "    else:\n",
    "        print(\"Epoch {} - Training loss: {}\".format(e, running_loss/len(val)))\n",
    "\n",
    "print(\"\\nTraining Time (in minutes) =\",(time()-time0)/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number Of Images Tested = 90\n",
      "\n",
      "Model Accuracy = 0.5333333333333333\n"
     ]
    }
   ],
   "source": [
    "correct_count, all_count = 0, 0    \n",
    "\n",
    "for i in range(len(te_val)):\n",
    "    temp_1 = [te_val[i]]\n",
    "    temp0 = np.array(temp_1)\n",
    "    temp = torch.from_numpy(temp0)\n",
    "    temp1 = temp.float()\n",
    "    with torch.no_grad():\n",
    "        logps = model(temp1)\n",
    "    ps = torch.exp(logps)\n",
    "    probab = list(ps.numpy()[0])\n",
    "    pred_label = probab.index(max(probab))\n",
    "    #print(pred_label)\n",
    "    if pred_label == 0:\n",
    "        fin_label = 0\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            d_logps = d_model(temp1) #set temp1 corr\n",
    "        d_ps = torch.exp(d_logps)\n",
    "        d_probab = list(d_ps.numpy()[0])\n",
    "        d_pred_label = d_probab.index(max(d_probab))\n",
    "        fin_label = d_pred_label + 1\n",
    "    \n",
    "    true_label = te_keys[i]\n",
    "    if(true_label == fin_label):\n",
    "        correct_count += 1      \n",
    "    all_count += 1\n",
    "\n",
    "print(\"Number Of Images Tested =\", all_count)\n",
    "print(\"\\nModel Accuracy =\", (correct_count/all_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correct_count, all_count = 0, 0    \n",
    "# norm_c, norm_true = 0,0\n",
    "# dis_c, dis_true = 0,0\n",
    "\n",
    "# for i in range(len(te_val)):\n",
    "#     temp_1 = [te_val[i]]\n",
    "#     temp0 = np.array(temp_1)\n",
    "#     temp = torch.from_numpy(temp0)\n",
    "#     temp1 = temp.float()\n",
    "#     with torch.no_grad():\n",
    "#         logps = model(temp1)\n",
    "#     ps = torch.exp(logps)\n",
    "#     probab = list(ps.numpy()[0])\n",
    "#     pred_label = probab.index(max(probab))\n",
    "#     true_label = te_keys[i]\n",
    "#     if(true_label == pred_label):\n",
    "#         correct_count += 1\n",
    "#         if true_label == 0:\n",
    "#             norm_true += 1\n",
    "#         else:\n",
    "#             dis_true += 1\n",
    "#     if true_label == 0:\n",
    "#         norm_c += 1\n",
    "#     else:\n",
    "#         dis_c += 1        \n",
    "#     all_count += 1\n",
    "\n",
    "# print(\"Number Of Images Tested =\", all_count)\n",
    "# print(\"\\nModel Accuracy =\", (correct_count/all_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#true-false 80 percent\n",
    "# print(\"Number Of norm =\", norm_c)\n",
    "# print(\"\\nModel Accuracy norm=\", (norm_true/norm_c))\n",
    "# print(\"Number Of dis =\", dis_c)\n",
    "# print(\"\\nModel Accuracy dis=\", (dis_true/dis_c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, './my_mnist_model.pt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
