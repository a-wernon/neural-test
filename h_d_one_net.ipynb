{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "from torchvision import datasets, transforms\n",
    "from torch import nn, optim\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 0.0, 0.0, 0.0, 1, 0.0, 0.0, 0.0, 1, 0.0, 0.0, 1, 0.0, 1, 1, 1, 1, 0.0, 0.0, 1, 1, 0.0, 0.0, 0.0, 1, 1, 0.0, 1, 1, 1, 1, 0.0, 1, 0.0, 1, 0.0, 1, 0.0, 0.0, 0.0, 1, 0.0, 1, 0.0, 0.0, 0.0, 0.0, 1, 1, 0.0, 0.0, 0.0, 1, 0.0, 1, 0.0, 1, 0.0, 0.0, 0.0, 0.0, 1, 1, 1, 0.0, 1, 0.0, 0.0, 0.0, 1, 0.0, 0.0, 1, 1, 1, 1, 1, 1, 1, 0.0, 0.0, 0.0, 0.0, 1, 1, 1, 0.0, 1, 0.0, 0.0, 1, 0.0, 0.0, 0.0, 1, 1, 0.0, 1, 0.0, 0.0, 0.0, 0.0, 1, 1, 1, 1, 0.0, 0.0, 0.0, 1, 1, 1, 1, 0.0, 1, 0.0, 0.0, 1, 0.0, 0.0, 0.0, 0.0, 1, 0.0, 0.0, 0.0, 0.0, 0.0, 1, 0.0, 1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1, 0.0, 0.0, 0.0, 0.0, 1, 1, 1, 0.0, 0.0, 0.0, 0.0, 0.0, 1, 0.0, 1, 1, 0.0, 0.0, 0.0, 0.0, 1, 1, 1, 0.0, 0.0, 0.0, 1, 1, 1, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1, 0.0, 1, 1, 1, 0.0, 1, 0.0, 0.0, 0.0, 1, 1, 1, 0.0, 1, 1, 1, 1, 0.0, 1, 0.0, 1, 1, 1, 1, 1, 0.0, 1, 0.0, 1, 0.0, 0.0, 0.0, 1, 0.0, 1, 0.0, 1, 0.0, 0.0, 1, 1, 1, 1, 1, 0.0, 1, 0.0, 0.0, 0.0, 0.0, 1, 0.0, 1, 0.0, 1, 1, 0.0, 1, 0.0, 1, 1, 0.0, 0.0, 1, 1, 1, 1, 0.0, 0.0, 1, 0.0, 1, 0.0, 1, 0.0, 1, 0.0, 0.0, 1, 1, 0.0, 1, 1, 1, 0.0, 0.0, 0.0, 1, 0.0, 0.0, 0.0, 1, 0.0, 0.0, 0.0, 1, 1, 1, 1, 1, 0.0, 0.0, 0.0, 0.0, 1, 1, 0.0, 0.0, 0.0, 0.0, 1, 0.0, 0.0, 1, 0.0, 1, 0.0, 1, 1, 0.0, 1]\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,)),])\n",
    "a = open('processed.cleveland.data', 'r')\n",
    "c = a.readlines()\n",
    "random.shuffle(c) #shuffle data before using\n",
    "n_val = []\n",
    "n_keys = []\n",
    "d = []\n",
    "for elem in c:\n",
    "    elem = elem[:-1]\n",
    "    elem = list(map(float,elem.split(',')))\n",
    "    n_val.append(elem[:-1])\n",
    "    n_keys.append(elem[-1]) #mb ad\n",
    "rot = [[] for i in range(len(n_val[0]))]\n",
    "mean = []\n",
    "std = []\n",
    "for i in range(len(n_val)):\n",
    "    for j in range(len(n_val[i])):\n",
    "        rot[j].append(n_val[i][j])\n",
    "#print(len(rot[0]))\n",
    "for elem in rot:\n",
    "    #print(elem)\n",
    "    new_elem = np.array(elem)\n",
    "    #print(np.std(new_elem))\n",
    "    #print(new_elem.shape)\n",
    "    std.append(np.std(new_elem))\n",
    "    mean.append(np.mean(new_elem))\n",
    "    #elem = [int(elem[i]*10) for i in range(len(elem))] \n",
    "for i in range(len(rot)):\n",
    "    rot[i] = [(rot[i][j] - mean[i])/std[i] for j in range(len(elem))]\n",
    "#print(rot[0])\n",
    "rot = np.array(rot)\n",
    "n_val = rot.T     #all of the previous 20 lines normalises our array\n",
    "\n",
    "\n",
    "#true-false\n",
    "for i in range(len(n_keys)):\n",
    "    if n_keys[i] > 0:\n",
    "        n_keys[i] = 1\n",
    "print(n_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(225, 13)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#73 - test size\n",
    "#230 - train size\n",
    "t = 230\n",
    "te = 73\n",
    "a = random.sample(range(303), t)\n",
    "val, keys, te_val, te_keys = [],[],[],[]\n",
    "for i in range(len(n_val)):\n",
    "    if i in a:\n",
    "        val.append(n_val[i])\n",
    "        keys.append(n_keys[i])\n",
    "    else:\n",
    "        te_val.append(n_val[i])                       #train-test \n",
    "        te_keys.append(n_keys[i])\n",
    "\n",
    "val = np.array(val)\n",
    "keys = np.array(keys)\n",
    "#val = val.astype(float)\n",
    "#keys = keys.astype(float)\n",
    "\n",
    "te_val = np.array(te_val)\n",
    "te_keys = np.array(te_keys)\n",
    "#keys.shape\n",
    "val.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Linear(in_features=13, out_features=128, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "  (3): ReLU()\n",
      "  (4): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (5): ReLU()\n",
      "  (6): Linear(in_features=64, out_features=2, bias=True)\n",
      "  (7): LogSoftmax()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "input_size = 13\n",
    "hidden_sizes = [128, 128, 64, 32]\n",
    "output_size = 2\n",
    "\n",
    "model = nn.Sequential(nn.Linear(input_size, hidden_sizes[0]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[1], hidden_sizes[2]),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(hidden_sizes[2], output_size),\n",
    "                      nn.LogSoftmax(dim=1))\n",
    "print(model) #model with 3 layers, first 2 with relu, last with logistic curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss() #MSELoss() \n",
    "#images, labels = next(iter(trainloader))\n",
    "#images = images.view(images.shape[0], -1)\n",
    "\n",
    "#logps = model(images) #log probabilities\n",
    "#loss = criterion(logps, labels) #calculate the NLL loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - Training loss: 0.6876393225457933\n",
      "Epoch 1 - Training loss: 0.6801290570365058\n",
      "Epoch 2 - Training loss: 0.6727609909905328\n",
      "Epoch 3 - Training loss: 0.6650263102849324\n",
      "Epoch 4 - Training loss: 0.6564160231749216\n",
      "Epoch 5 - Training loss: 0.6465616533491346\n",
      "Epoch 6 - Training loss: 0.6350971688164605\n",
      "Epoch 7 - Training loss: 0.6217373104890188\n",
      "Epoch 8 - Training loss: 0.6062392066584693\n",
      "Epoch 9 - Training loss: 0.5885203393300374\n",
      "Epoch 10 - Training loss: 0.5685980904764599\n",
      "Epoch 11 - Training loss: 0.5467417520284653\n",
      "Epoch 12 - Training loss: 0.5235792314343982\n",
      "Epoch 13 - Training loss: 0.4997875518931283\n",
      "Epoch 14 - Training loss: 0.47654144687785044\n",
      "Epoch 15 - Training loss: 0.4547892172469033\n",
      "Epoch 16 - Training loss: 0.43531724290715323\n",
      "Epoch 17 - Training loss: 0.4184152474006017\n",
      "Epoch 18 - Training loss: 0.4041556124885877\n",
      "Epoch 19 - Training loss: 0.39220831740233636\n",
      "Epoch 20 - Training loss: 0.38224096538292035\n",
      "Epoch 21 - Training loss: 0.3738003060304456\n",
      "Epoch 22 - Training loss: 0.36646587582925955\n",
      "Epoch 23 - Training loss: 0.3600785311145915\n",
      "Epoch 24 - Training loss: 0.35437269436816377\n",
      "Epoch 25 - Training loss: 0.3492483206424448\n",
      "Epoch 26 - Training loss: 0.34445426378813054\n",
      "Epoch 27 - Training loss: 0.33996992636471984\n",
      "Epoch 28 - Training loss: 0.33565130138148863\n",
      "Epoch 29 - Training loss: 0.33155426158673235\n",
      "Epoch 30 - Training loss: 0.327552614543173\n",
      "Epoch 31 - Training loss: 0.32359646225968997\n",
      "Epoch 32 - Training loss: 0.319774927397569\n",
      "Epoch 33 - Training loss: 0.3160463164622585\n",
      "Epoch 34 - Training loss: 0.3123560091604789\n",
      "Epoch 35 - Training loss: 0.3086924035102129\n",
      "Epoch 36 - Training loss: 0.3050324649901854\n",
      "Epoch 37 - Training loss: 0.30144927132874727\n",
      "Epoch 38 - Training loss: 0.2978297063832482\n",
      "Epoch 39 - Training loss: 0.29421238304012354\n",
      "Epoch 40 - Training loss: 0.29060669453400706\n",
      "Epoch 41 - Training loss: 0.28701424561647904\n",
      "Epoch 42 - Training loss: 0.28337373656324216\n",
      "Epoch 43 - Training loss: 0.27970665904382863\n",
      "Epoch 44 - Training loss: 0.27604552107345726\n",
      "Epoch 45 - Training loss: 0.27238100932911036\n",
      "Epoch 46 - Training loss: 0.2686147173059483\n",
      "Epoch 47 - Training loss: 0.2648989359682633\n",
      "Epoch 48 - Training loss: 0.26123487733821904\n",
      "Epoch 49 - Training loss: 0.2574992047912545\n",
      "Epoch 50 - Training loss: 0.2538180467165593\n",
      "Epoch 51 - Training loss: 0.25016886450246806\n",
      "Epoch 52 - Training loss: 0.246357674366898\n",
      "Epoch 53 - Training loss: 0.24267352361852923\n",
      "Epoch 54 - Training loss: 0.23897256917527152\n",
      "Epoch 55 - Training loss: 0.23525250964798033\n",
      "Epoch 56 - Training loss: 0.23147166163350144\n",
      "Epoch 57 - Training loss: 0.22799515794031322\n",
      "Epoch 58 - Training loss: 0.22422987593027452\n",
      "Epoch 59 - Training loss: 0.2205293090093053\n",
      "Epoch 60 - Training loss: 0.2169067404833105\n",
      "Epoch 61 - Training loss: 0.21320323686529366\n",
      "Epoch 62 - Training loss: 0.20962097993948392\n",
      "Epoch 63 - Training loss: 0.20611675263434234\n",
      "Epoch 64 - Training loss: 0.20256989523613206\n",
      "Epoch 65 - Training loss: 0.19907025096161912\n",
      "Epoch 66 - Training loss: 0.19560058726359986\n",
      "Epoch 67 - Training loss: 0.1921744961519208\n",
      "Epoch 68 - Training loss: 0.18872999221396944\n",
      "Epoch 69 - Training loss: 0.18543326521240588\n",
      "Epoch 70 - Training loss: 0.18204157886276434\n",
      "Epoch 71 - Training loss: 0.1787320327339694\n",
      "Epoch 72 - Training loss: 0.17545641374851886\n",
      "Epoch 73 - Training loss: 0.172199195396776\n",
      "Epoch 74 - Training loss: 0.1689890580632103\n",
      "Epoch 75 - Training loss: 0.16586159098285458\n",
      "Epoch 76 - Training loss: 0.1626688554312568\n",
      "Epoch 77 - Training loss: 0.1596425402420573\n",
      "Epoch 78 - Training loss: 0.15657955173144325\n",
      "Epoch 79 - Training loss: 0.15356786485011173\n",
      "Epoch 80 - Training loss: 0.15049139111587365\n",
      "Epoch 81 - Training loss: 0.14752335201858335\n",
      "Epoch 82 - Training loss: 0.14461569808798636\n",
      "Epoch 83 - Training loss: 0.1417289463545118\n",
      "Epoch 84 - Training loss: 0.13885413153219917\n",
      "Epoch 85 - Training loss: 0.1359788148635481\n",
      "Epoch 86 - Training loss: 0.13328587145816223\n",
      "Epoch 87 - Training loss: 0.13055648448514856\n",
      "Epoch 88 - Training loss: 0.1277414763668397\n",
      "Epoch 89 - Training loss: 0.1249892973363684\n",
      "Epoch 90 - Training loss: 0.12221113101831482\n",
      "Epoch 91 - Training loss: 0.1195309246500579\n",
      "Epoch 92 - Training loss: 0.11699162725892771\n",
      "Epoch 93 - Training loss: 0.11422770260509828\n",
      "Epoch 94 - Training loss: 0.11182157098778084\n",
      "Epoch 95 - Training loss: 0.10912615095132626\n",
      "Epoch 96 - Training loss: 0.1065913347622619\n",
      "Epoch 97 - Training loss: 0.10416976900716286\n",
      "Epoch 98 - Training loss: 0.10169481367516306\n",
      "Epoch 99 - Training loss: 0.09922956136451426\n",
      "Epoch 100 - Training loss: 0.09655497745942233\n",
      "Epoch 101 - Training loss: 0.09413076357917614\n",
      "Epoch 102 - Training loss: 0.09143781895278759\n",
      "Epoch 103 - Training loss: 0.08904528450127044\n",
      "Epoch 104 - Training loss: 0.08651515612892884\n",
      "Epoch 105 - Training loss: 0.08413864624694624\n",
      "Epoch 106 - Training loss: 0.08195986012354195\n",
      "Epoch 107 - Training loss: 0.07945597781103667\n",
      "Epoch 108 - Training loss: 0.07732229821560598\n",
      "Epoch 109 - Training loss: 0.07512743135542728\n",
      "Epoch 110 - Training loss: 0.07294253468729746\n",
      "Epoch 111 - Training loss: 0.07087527337126731\n",
      "Epoch 112 - Training loss: 0.06883248200431101\n",
      "Epoch 113 - Training loss: 0.06675708667401421\n",
      "Epoch 114 - Training loss: 0.06478003086564463\n",
      "Epoch 115 - Training loss: 0.06282708538401696\n",
      "Epoch 116 - Training loss: 0.061009656496941414\n",
      "Epoch 117 - Training loss: 0.059146969731753214\n",
      "Epoch 118 - Training loss: 0.057189705166630224\n",
      "Epoch 119 - Training loss: 0.055350540357942214\n",
      "Epoch 120 - Training loss: 0.05374893220675201\n",
      "Epoch 121 - Training loss: 0.05203945510813456\n",
      "Epoch 122 - Training loss: 0.05044349684921346\n",
      "Epoch 123 - Training loss: 0.048996407616978606\n",
      "Epoch 124 - Training loss: 0.047534252791084426\n",
      "Epoch 125 - Training loss: 0.046090559983848735\n",
      "Epoch 126 - Training loss: 0.04465759819751181\n",
      "Epoch 127 - Training loss: 0.0434265773452095\n",
      "Epoch 128 - Training loss: 0.04206667695298165\n",
      "Epoch 129 - Training loss: 0.040800654331478906\n",
      "Epoch 130 - Training loss: 0.039625885243821005\n",
      "Epoch 131 - Training loss: 0.03850557364203117\n",
      "Epoch 132 - Training loss: 0.03735286608638312\n",
      "Epoch 133 - Training loss: 0.036278441710424485\n",
      "Epoch 134 - Training loss: 0.03514222339483606\n",
      "Epoch 135 - Training loss: 0.034283743281083114\n",
      "Epoch 136 - Training loss: 0.033219951732658425\n",
      "Epoch 137 - Training loss: 0.0322855540999208\n",
      "Epoch 138 - Training loss: 0.03147099018521756\n",
      "Epoch 139 - Training loss: 0.030598458556983354\n",
      "Epoch 140 - Training loss: 0.029621970780326686\n",
      "Epoch 141 - Training loss: 0.028891434380697564\n",
      "Epoch 142 - Training loss: 0.02814525677883198\n",
      "Epoch 143 - Training loss: 0.02724056823707097\n",
      "Epoch 144 - Training loss: 0.026620731226500077\n",
      "Epoch 145 - Training loss: 0.025849501530880475\n",
      "Epoch 146 - Training loss: 0.025089360163395833\n",
      "Epoch 147 - Training loss: 0.02447877512361575\n",
      "Epoch 148 - Training loss: 0.023851115764839725\n",
      "Epoch 149 - Training loss: 0.023118617875352958\n",
      "Epoch 150 - Training loss: 0.0225232314578348\n",
      "Epoch 151 - Training loss: 0.021995397101793955\n",
      "Epoch 152 - Training loss: 0.021360999242043748\n",
      "Epoch 153 - Training loss: 0.020875945876091748\n",
      "Epoch 154 - Training loss: 0.020357853810387874\n",
      "Epoch 155 - Training loss: 0.019816061690699296\n",
      "Epoch 156 - Training loss: 0.019306638516156025\n",
      "Epoch 157 - Training loss: 0.018842839961620287\n",
      "Epoch 158 - Training loss: 0.018400352187374267\n",
      "Epoch 159 - Training loss: 0.017965514818700495\n",
      "Epoch 160 - Training loss: 0.01748925715167581\n",
      "Epoch 161 - Training loss: 0.017117089088626126\n",
      "Epoch 162 - Training loss: 0.016707956248001653\n",
      "Epoch 163 - Training loss: 0.01630811972727991\n",
      "Epoch 164 - Training loss: 0.015940580890760652\n",
      "Epoch 165 - Training loss: 0.015581713501437883\n",
      "Epoch 166 - Training loss: 0.015240273144465593\n",
      "Epoch 167 - Training loss: 0.014899542448880412\n",
      "Epoch 168 - Training loss: 0.014562422900534229\n",
      "Epoch 169 - Training loss: 0.014246590089490939\n",
      "Epoch 170 - Training loss: 0.013943939226004811\n",
      "Epoch 171 - Training loss: 0.013655567143469449\n",
      "Epoch 172 - Training loss: 0.013319286137866584\n",
      "Epoch 173 - Training loss: 0.013098609628133697\n",
      "Epoch 174 - Training loss: 0.012815027684984746\n",
      "Epoch 175 - Training loss: 0.01253623809128928\n",
      "Epoch 176 - Training loss: 0.012282221544291917\n",
      "Epoch 177 - Training loss: 0.012039100734038407\n",
      "Epoch 178 - Training loss: 0.011787791160765108\n",
      "Epoch 179 - Training loss: 0.011559258282754727\n",
      "Epoch 180 - Training loss: 0.011374511163838861\n",
      "Epoch 181 - Training loss: 0.011114390528290666\n",
      "Epoch 182 - Training loss: 0.010919564988977552\n",
      "Epoch 183 - Training loss: 0.010685870685810938\n",
      "Epoch 184 - Training loss: 0.010521561162208697\n",
      "Epoch 185 - Training loss: 0.010309493355759305\n",
      "Epoch 186 - Training loss: 0.0101268556893103\n",
      "Epoch 187 - Training loss: 0.009933178577736738\n",
      "Epoch 188 - Training loss: 0.009760397761148076\n",
      "Epoch 189 - Training loss: 0.009592563244392175\n",
      "Epoch 190 - Training loss: 0.009416440182692725\n",
      "Epoch 191 - Training loss: 0.00926134520534551\n",
      "Epoch 192 - Training loss: 0.009080219650048902\n",
      "Epoch 193 - Training loss: 0.00894903778408055\n",
      "Epoch 194 - Training loss: 0.008803301039040687\n",
      "Epoch 195 - Training loss: 0.008640206111104665\n",
      "Epoch 196 - Training loss: 0.008506968224673478\n",
      "Epoch 197 - Training loss: 0.008359958053587427\n",
      "Epoch 198 - Training loss: 0.008239408883142403\n",
      "Epoch 199 - Training loss: 0.008109689368367867\n",
      "Epoch 200 - Training loss: 0.007969512017969199\n",
      "Epoch 201 - Training loss: 0.00785307404882\n",
      "Epoch 202 - Training loss: 0.007739864287988875\n",
      "Epoch 203 - Training loss: 0.007615081346500258\n",
      "Epoch 204 - Training loss: 0.007492904654912296\n",
      "Epoch 205 - Training loss: 0.0073863196674783345\n",
      "Epoch 206 - Training loss: 0.007275108030067751\n",
      "Epoch 207 - Training loss: 0.007173302567620175\n",
      "Epoch 208 - Training loss: 0.00706157713456624\n",
      "Epoch 209 - Training loss: 0.006959944197038636\n",
      "Epoch 210 - Training loss: 0.006865994188061772\n",
      "Epoch 211 - Training loss: 0.006763356700895618\n",
      "Epoch 212 - Training loss: 0.006670793613636066\n",
      "Epoch 213 - Training loss: 0.006585617097848058\n",
      "Epoch 214 - Training loss: 0.006498076726050625\n",
      "Epoch 215 - Training loss: 0.0063905621274747346\n",
      "Epoch 216 - Training loss: 0.00631492643244766\n",
      "Epoch 217 - Training loss: 0.006228272424585144\n",
      "Epoch 218 - Training loss: 0.006140389196509849\n",
      "Epoch 219 - Training loss: 0.006065470411397516\n",
      "Epoch 220 - Training loss: 0.005977823763331003\n",
      "Epoch 221 - Training loss: 0.0059051222093563635\n",
      "Epoch 222 - Training loss: 0.005829862003238999\n",
      "Epoch 223 - Training loss: 0.005753623345396982\n",
      "Epoch 224 - Training loss: 0.005675296870687229\n",
      "Epoch 225 - Training loss: 0.005605197702283091\n",
      "Epoch 226 - Training loss: 0.005539047645789595\n",
      "Epoch 227 - Training loss: 0.0054664786375344145\n",
      "Epoch 228 - Training loss: 0.005400404890889629\n",
      "Epoch 229 - Training loss: 0.005330182048933144\n",
      "Epoch 230 - Training loss: 0.0052699594855362695\n",
      "Epoch 231 - Training loss: 0.0052028069193001\n",
      "Epoch 232 - Training loss: 0.005142931531777448\n",
      "Epoch 233 - Training loss: 0.0050775436860322366\n",
      "Epoch 234 - Training loss: 0.005017899885950151\n",
      "Epoch 235 - Training loss: 0.004966355631143307\n",
      "Epoch 236 - Training loss: 0.004902392621682231\n",
      "Epoch 237 - Training loss: 0.00484274543473232\n",
      "Epoch 238 - Training loss: 0.0047915067686278056\n",
      "Epoch 239 - Training loss: 0.004735139809543638\n",
      "Epoch 240 - Training loss: 0.004684002284450028\n",
      "Epoch 241 - Training loss: 0.004624300079245371\n",
      "Epoch 242 - Training loss: 0.004575065130091913\n",
      "Epoch 243 - Training loss: 0.004526017179316884\n",
      "Epoch 244 - Training loss: 0.004472456460781776\n",
      "Epoch 245 - Training loss: 0.004424233525746091\n",
      "Epoch 246 - Training loss: 0.004371621015677962\n",
      "Epoch 247 - Training loss: 0.004326998395441642\n",
      "Epoch 248 - Training loss: 0.0042765093012766585\n",
      "Epoch 249 - Training loss: 0.00423631277394059\n",
      "Epoch 250 - Training loss: 0.004184680023562287\n",
      "Epoch 251 - Training loss: 0.004138333598392528\n",
      "Epoch 252 - Training loss: 0.004099240014776121\n",
      "Epoch 253 - Training loss: 0.00405179468264243\n",
      "Epoch 254 - Training loss: 0.004012402726607244\n",
      "Epoch 255 - Training loss: 0.0039664011200039795\n",
      "Epoch 256 - Training loss: 0.003928487713091992\n",
      "Epoch 257 - Training loss: 0.0038885303056830226\n",
      "Epoch 258 - Training loss: 0.0038447548757246054\n",
      "Epoch 259 - Training loss: 0.003810917649457129\n",
      "Epoch 260 - Training loss: 0.003769696393496215\n",
      "Epoch 261 - Training loss: 0.0037307652788178794\n",
      "Epoch 262 - Training loss: 0.0036972695158989824\n",
      "Epoch 263 - Training loss: 0.0036569221976179103\n",
      "Epoch 264 - Training loss: 0.003621134032581368\n",
      "Epoch 265 - Training loss: 0.003588406803325294\n",
      "Epoch 266 - Training loss: 0.0035559062981370617\n",
      "Epoch 267 - Training loss: 0.003519855542248662\n",
      "Epoch 268 - Training loss: 0.0034851148187603244\n",
      "Epoch 269 - Training loss: 0.0034559918254709033\n",
      "Epoch 270 - Training loss: 0.0034231446453169825\n",
      "Epoch 271 - Training loss: 0.00338738581453858\n",
      "Epoch 272 - Training loss: 0.00335819153649813\n",
      "Epoch 273 - Training loss: 0.003329593829684383\n",
      "Epoch 274 - Training loss: 0.0032971829090613545\n",
      "Epoch 275 - Training loss: 0.0032695310317286106\n",
      "Epoch 276 - Training loss: 0.0032383623399013257\n",
      "Epoch 277 - Training loss: 0.0032081986733963203\n",
      "Epoch 278 - Training loss: 0.0031830923592941138\n",
      "Epoch 279 - Training loss: 0.003150734422469049\n",
      "Epoch 280 - Training loss: 0.0031246015982494177\n",
      "Epoch 281 - Training loss: 0.003098992042785527\n",
      "Epoch 282 - Training loss: 0.003071155628581121\n",
      "Epoch 283 - Training loss: 0.0030437221510747274\n",
      "Epoch 284 - Training loss: 0.003017669365474976\n",
      "Epoch 285 - Training loss: 0.002996868247699385\n",
      "Epoch 286 - Training loss: 0.00296648556741161\n",
      "Epoch 287 - Training loss: 0.002943126315893778\n",
      "Epoch 288 - Training loss: 0.002917352986196799\n",
      "Epoch 289 - Training loss: 0.002896206475694751\n",
      "Epoch 290 - Training loss: 0.0028673746605322016\n",
      "Epoch 291 - Training loss: 0.002849129109681725\n",
      "Epoch 292 - Training loss: 0.0028230148692711685\n",
      "Epoch 293 - Training loss: 0.0027994979581867736\n",
      "Epoch 294 - Training loss: 0.0027796819358015847\n",
      "Epoch 295 - Training loss: 0.0027537165632578347\n",
      "Epoch 296 - Training loss: 0.0027336051527574847\n",
      "Epoch 297 - Training loss: 0.00271213597026095\n",
      "Epoch 298 - Training loss: 0.00268882797183835\n",
      "Epoch 299 - Training loss: 0.0026701653082940565\n",
      "Epoch 300 - Training loss: 0.0026496951821320005\n",
      "Epoch 301 - Training loss: 0.0026286590239973547\n",
      "Epoch 302 - Training loss: 0.00260496064409665\n",
      "Epoch 303 - Training loss: 0.002589698726413903\n",
      "Epoch 304 - Training loss: 0.0025663781529355685\n",
      "Epoch 305 - Training loss: 0.002547167115036504\n",
      "Epoch 306 - Training loss: 0.0025290750666027545\n",
      "Epoch 307 - Training loss: 0.002509474679840101\n",
      "Epoch 308 - Training loss: 0.002490806701069314\n",
      "Epoch 309 - Training loss: 0.002474106649493106\n",
      "Epoch 310 - Training loss: 0.00245370893559845\n",
      "Epoch 311 - Training loss: 0.002436638942118792\n",
      "Epoch 312 - Training loss: 0.0024166034808928314\n",
      "Epoch 313 - Training loss: 0.002399258500722649\n",
      "Epoch 314 - Training loss: 0.0023849302804359873\n",
      "Epoch 315 - Training loss: 0.0023645767724906363\n",
      "Epoch 316 - Training loss: 0.0023482217028673504\n",
      "Epoch 317 - Training loss: 0.0023326977408397436\n",
      "Epoch 318 - Training loss: 0.002315500126369664\n",
      "Epoch 319 - Training loss: 0.0023001543015705454\n",
      "Epoch 320 - Training loss: 0.0022804512503971828\n",
      "Epoch 321 - Training loss: 0.0022664904020724447\n",
      "Epoch 322 - Training loss: 0.0022519118156183833\n",
      "Epoch 323 - Training loss: 0.002234460918064056\n",
      "Epoch 324 - Training loss: 0.0022195157804332805\n",
      "Epoch 325 - Training loss: 0.002204534672324314\n",
      "Epoch 326 - Training loss: 0.0021910730238266438\n",
      "Epoch 327 - Training loss: 0.0021741574749801393\n",
      "Epoch 328 - Training loss: 0.002159250614840952\n",
      "Epoch 329 - Training loss: 0.0021461989669482287\n",
      "Epoch 330 - Training loss: 0.0021300807052899826\n",
      "Epoch 331 - Training loss: 0.0021178638568061263\n",
      "Epoch 332 - Training loss: 0.002102269369647603\n",
      "Epoch 333 - Training loss: 0.0020881775756276966\n",
      "Epoch 334 - Training loss: 0.002076275905369711\n",
      "Epoch 335 - Training loss: 0.0020614257153714435\n",
      "Epoch 336 - Training loss: 0.0020473094216475196\n",
      "Epoch 337 - Training loss: 0.002035846254588149\n",
      "Epoch 338 - Training loss: 0.002022560868956336\n",
      "Epoch 339 - Training loss: 0.0020081022572690523\n",
      "Epoch 340 - Training loss: 0.0019956907361019047\n",
      "Epoch 341 - Training loss: 0.0019851626515791547\n",
      "Epoch 342 - Training loss: 0.0019706321487886827\n",
      "Epoch 343 - Training loss: 0.001958185000562354\n",
      "Epoch 344 - Training loss: 0.00194647973326596\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 345 - Training loss: 0.0019335973093523586\n",
      "Epoch 346 - Training loss: 0.0019231696577829086\n",
      "Epoch 347 - Training loss: 0.0019088546735716882\n",
      "Epoch 348 - Training loss: 0.0018998300146203127\n",
      "Epoch 349 - Training loss: 0.0018859403335129818\n",
      "Epoch 350 - Training loss: 0.0018752228321871043\n",
      "Epoch 351 - Training loss: 0.0018639615885267505\n",
      "Epoch 352 - Training loss: 0.0018530467563603124\n",
      "Epoch 353 - Training loss: 0.0018412409689070664\n",
      "Epoch 354 - Training loss: 0.001830225160438993\n",
      "Epoch 355 - Training loss: 0.0018198121556475004\n",
      "Epoch 356 - Training loss: 0.0018099434168282061\n",
      "Epoch 357 - Training loss: 0.0017971718039011206\n",
      "Epoch 358 - Training loss: 0.0017874461913525075\n",
      "Epoch 359 - Training loss: 0.0017772617991162873\n",
      "Epoch 360 - Training loss: 0.0017663415598856127\n",
      "Epoch 361 - Training loss: 0.001755893951043169\n",
      "Epoch 362 - Training loss: 0.0017460358754939243\n",
      "Epoch 363 - Training loss: 0.0017353479268545078\n",
      "Epoch 364 - Training loss: 0.0017266061999161606\n",
      "Epoch 365 - Training loss: 0.0017165354275727887\n",
      "Epoch 366 - Training loss: 0.001705307484419494\n",
      "Epoch 367 - Training loss: 0.0016979550978850474\n",
      "Epoch 368 - Training loss: 0.0016862745952683944\n",
      "Epoch 369 - Training loss: 0.0016772535581203681\n",
      "Epoch 370 - Training loss: 0.001667951938150749\n",
      "Epoch 371 - Training loss: 0.0016585923364572687\n",
      "Epoch 372 - Training loss: 0.001649045769675455\n",
      "Epoch 373 - Training loss: 0.001640741048116057\n",
      "Epoch 374 - Training loss: 0.001631167585168656\n",
      "Epoch 375 - Training loss: 0.001622586129087564\n",
      "Epoch 376 - Training loss: 0.0016128484302802804\n",
      "Epoch 377 - Training loss: 0.0016043344795256252\n",
      "Epoch 378 - Training loss: 0.0015963498394339125\n",
      "Epoch 379 - Training loss: 0.0015879082440535639\n",
      "Epoch 380 - Training loss: 0.0015789797262352875\n",
      "Epoch 381 - Training loss: 0.0015700223677430358\n",
      "Epoch 382 - Training loss: 0.0015627286516096699\n",
      "Epoch 383 - Training loss: 0.001554298575466204\n",
      "Epoch 384 - Training loss: 0.0015456659609460151\n",
      "Epoch 385 - Training loss: 0.0015386585615479792\n",
      "Epoch 386 - Training loss: 0.0015295633907927847\n",
      "Epoch 387 - Training loss: 0.0015210938088071973\n",
      "Epoch 388 - Training loss: 0.0015139518325614492\n",
      "Epoch 389 - Training loss: 0.0015065941370853075\n",
      "Epoch 390 - Training loss: 0.0014983063047364137\n",
      "Epoch 391 - Training loss: 0.0014910120558647578\n",
      "Epoch 392 - Training loss: 0.0014831233887667218\n",
      "Epoch 393 - Training loss: 0.0014756265539138743\n",
      "Epoch 394 - Training loss: 0.0014684332355232577\n",
      "Epoch 395 - Training loss: 0.001460581865030185\n",
      "Epoch 396 - Training loss: 0.0014538544365442856\n",
      "Epoch 397 - Training loss: 0.0014465994090459035\n",
      "Epoch 398 - Training loss: 0.001438413964807776\n",
      "Epoch 399 - Training loss: 0.0014320609161570196\n",
      "Epoch 400 - Training loss: 0.0014244628660108245\n",
      "Epoch 401 - Training loss: 0.0014179834228713792\n",
      "Epoch 402 - Training loss: 0.001410606137361583\n",
      "Epoch 403 - Training loss: 0.0014040772047293458\n",
      "Epoch 404 - Training loss: 0.0013971868960158595\n",
      "Epoch 405 - Training loss: 0.0013894865164693075\n",
      "Epoch 406 - Training loss: 0.0013839397782452224\n",
      "Epoch 407 - Training loss: 0.001376760084992207\n",
      "Epoch 408 - Training loss: 0.0013704702104442074\n",
      "Epoch 409 - Training loss: 0.0013632763088138392\n",
      "Epoch 410 - Training loss: 0.0013573236663818496\n",
      "Epoch 411 - Training loss: 0.0013514085758704179\n",
      "Epoch 412 - Training loss: 0.0013437737220823149\n",
      "Epoch 413 - Training loss: 0.0013385272275732519\n",
      "Epoch 414 - Training loss: 0.0013315388607879363\n",
      "Epoch 415 - Training loss: 0.0013255540739704782\n",
      "Epoch 416 - Training loss: 0.001319281428147355\n",
      "Epoch 417 - Training loss: 0.00131299980398353\n",
      "Epoch 418 - Training loss: 0.0013074366148329217\n",
      "Epoch 419 - Training loss: 0.0013012436065304833\n",
      "Epoch 420 - Training loss: 0.0012954697518455305\n",
      "Epoch 421 - Training loss: 0.0012886427658194558\n",
      "Epoch 422 - Training loss: 0.0012833206072276463\n",
      "Epoch 423 - Training loss: 0.0012778816842693016\n",
      "Epoch 424 - Training loss: 0.0012717880854248797\n",
      "Epoch 425 - Training loss: 0.0012659959105875234\n",
      "Epoch 426 - Training loss: 0.0012605112181129621\n",
      "Epoch 427 - Training loss: 0.0012547670735598457\n",
      "Epoch 428 - Training loss: 0.0012485736088911558\n",
      "Epoch 429 - Training loss: 0.0012435402736765077\n",
      "Epoch 430 - Training loss: 0.001238538532261436\n",
      "Epoch 431 - Training loss: 0.0012317789266394483\n",
      "Epoch 432 - Training loss: 0.0012277118535932788\n",
      "Epoch 433 - Training loss: 0.001221637318208511\n",
      "Epoch 434 - Training loss: 0.0012164583309537925\n",
      "Epoch 435 - Training loss: 0.001211115461279822\n",
      "Epoch 436 - Training loss: 0.0012055823912025844\n",
      "Epoch 437 - Training loss: 0.001200347062107754\n",
      "Epoch 438 - Training loss: 0.001195476674993093\n",
      "Epoch 439 - Training loss: 0.0011903771384539\n",
      "Epoch 440 - Training loss: 0.0011848670534500153\n",
      "Epoch 441 - Training loss: 0.0011805402209133023\n",
      "Epoch 442 - Training loss: 0.001174975855546797\n",
      "Epoch 443 - Training loss: 0.0011701308540020881\n",
      "Epoch 444 - Training loss: 0.001164661566347181\n",
      "Epoch 445 - Training loss: 0.0011602837244171205\n",
      "Epoch 446 - Training loss: 0.0011554319525743987\n",
      "Epoch 447 - Training loss: 0.0011503595592363227\n",
      "Epoch 448 - Training loss: 0.0011452227856418378\n",
      "Epoch 449 - Training loss: 0.0011411163486438152\n",
      "Epoch 450 - Training loss: 0.0011357390154476989\n",
      "Epoch 451 - Training loss: 0.0011314638311840655\n",
      "Epoch 452 - Training loss: 0.0011267940498120134\n",
      "Epoch 453 - Training loss: 0.0011217977305552848\n",
      "Epoch 454 - Training loss: 0.001117372871060209\n",
      "Epoch 455 - Training loss: 0.0011126114699274265\n",
      "Epoch 456 - Training loss: 0.0011083720006678276\n",
      "Epoch 457 - Training loss: 0.0011040426406766655\n",
      "Epoch 458 - Training loss: 0.001099017344066308\n",
      "Epoch 459 - Training loss: 0.0010950702149638877\n",
      "Epoch 460 - Training loss: 0.0010905827416213595\n",
      "Epoch 461 - Training loss: 0.0010857396220713314\n",
      "Epoch 462 - Training loss: 0.0010817092180889177\n",
      "Epoch 463 - Training loss: 0.0010770986578732945\n",
      "Epoch 464 - Training loss: 0.0010730120351564033\n",
      "Epoch 465 - Training loss: 0.001069004146705639\n",
      "Epoch 466 - Training loss: 0.0010642060146161232\n",
      "Epoch 467 - Training loss: 0.0010604220302559498\n",
      "Epoch 468 - Training loss: 0.0010566098682558175\n",
      "Epoch 469 - Training loss: 0.0010515143423277904\n",
      "Epoch 470 - Training loss: 0.001048010760406694\n",
      "Epoch 471 - Training loss: 0.00104393737294332\n",
      "Epoch 472 - Training loss: 0.0010396295120148756\n",
      "Epoch 473 - Training loss: 0.0010357558554144615\n",
      "Epoch 474 - Training loss: 0.0010315376476589537\n",
      "Epoch 475 - Training loss: 0.0010275997335553357\n",
      "Epoch 476 - Training loss: 0.0010237773708345168\n",
      "Epoch 477 - Training loss: 0.0010195802682564981\n",
      "Epoch 478 - Training loss: 0.0010159406518577107\n",
      "Epoch 479 - Training loss: 0.0010120490025497966\n",
      "Epoch 480 - Training loss: 0.001008237163006211\n",
      "Epoch 481 - Training loss: 0.0010042538355144377\n",
      "Epoch 482 - Training loss: 0.0010004626296425399\n",
      "Epoch 483 - Training loss: 0.0009964635748197438\n",
      "Epoch 484 - Training loss: 0.0009935956194997984\n",
      "Epoch 485 - Training loss: 0.0009889783348111792\n",
      "Epoch 486 - Training loss: 0.0009857692532195806\n",
      "Epoch 487 - Training loss: 0.0009817292723660544\n",
      "Epoch 488 - Training loss: 0.0009782503665293045\n",
      "Epoch 489 - Training loss: 0.0009744093296028685\n",
      "Epoch 490 - Training loss: 0.0009711134884580019\n",
      "Epoch 491 - Training loss: 0.0009673262991385438\n",
      "Epoch 492 - Training loss: 0.0009638645998526841\n",
      "Epoch 493 - Training loss: 0.000960297177815461\n",
      "Epoch 494 - Training loss: 0.0009568567439882258\n",
      "Epoch 495 - Training loss: 0.0009531774550404748\n",
      "Epoch 496 - Training loss: 0.0009496457356351269\n",
      "Epoch 497 - Training loss: 0.0009466367389605226\n",
      "Epoch 498 - Training loss: 0.0009427126032587947\n",
      "Epoch 499 - Training loss: 0.0009394267752291638\n",
      "\n",
      "Training Time (in minutes) = 3.423677897453308\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.0005, momentum=0.6)\n",
    "time0 = time()\n",
    "epochs = 500\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for i in range(len(val)):\n",
    "        # Training pass\n",
    "        optimizer.zero_grad()\n",
    "        temp_1 = [val[i]]\n",
    "        temp0 = np.array(temp_1)\n",
    "        temp = torch.from_numpy(temp0)\n",
    "        temp1 = temp.float()\n",
    "        #print(temp1)\n",
    "        output = model(temp1)    #DOES NOT WORK. MODEL RETURNS NAN TENSOR\n",
    "        #print(output)\n",
    "        k = [keys[i]]\n",
    "        k = np.array(k)\n",
    "        k = torch.from_numpy(k)\n",
    "        k = k.long()\n",
    "        #print(output.shape)\n",
    "        #print(k.shape)\n",
    "        #print('result', e, output)\n",
    "        #print(k)\n",
    "        loss = criterion(output, k)\n",
    "        #print(loss)\n",
    "        \n",
    "        #This is where the model learns by backpropagating\n",
    "        loss.backward()\n",
    "        \n",
    "        #And optimizes its weights here\n",
    "        optimizer.step()\n",
    "        \n",
    "        #print(loss.item())\n",
    "        running_loss += loss.item()\n",
    "    else:\n",
    "        print(\"Epoch {} - Training loss: {}\".format(e, running_loss/len(val)))\n",
    "\n",
    "print(\"\\nTraining Time (in minutes) =\",(time()-time0)/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number Of Images Tested = 72\n",
      "\n",
      "Model Accuracy = 0.7777777777777778\n"
     ]
    }
   ],
   "source": [
    "correct_count, all_count = 0, 0    \n",
    "norm_c, norm_true = 0,0\n",
    "dis_c, dis_true = 0,0\n",
    "\n",
    "for i in range(len(te_val)):\n",
    "    temp_1 = [te_val[i]]\n",
    "    temp0 = np.array(temp_1)\n",
    "    temp = torch.from_numpy(temp0)\n",
    "    temp1 = temp.float()\n",
    "    with torch.no_grad():\n",
    "        logps = model(temp1)\n",
    "    ps = torch.exp(logps)\n",
    "    probab = list(ps.numpy()[0])\n",
    "    pred_label = probab.index(max(probab))\n",
    "    true_label = te_keys[i]\n",
    "    if(true_label == pred_label):\n",
    "        correct_count += 1\n",
    "        if true_label == 0:\n",
    "            norm_true += 1\n",
    "        else:\n",
    "            dis_true += 1\n",
    "    if true_label == 0:\n",
    "        norm_c += 1\n",
    "    else:\n",
    "        dis_c += 1        \n",
    "    all_count += 1\n",
    "\n",
    "print(\"Number Of Images Tested =\", all_count)\n",
    "print(\"\\nModel Accuracy =\", (correct_count/all_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number Of norm = 33\n",
      "\n",
      "Model Accuracy norm= 0.7575757575757576\n",
      "Number Of dis = 39\n",
      "\n",
      "Model Accuracy dis= 0.7948717948717948\n"
     ]
    }
   ],
   "source": [
    "#true-false 80 percent need 90\n",
    "print(\"Number Of norm =\", norm_c)\n",
    "print(\"\\nModel Accuracy norm=\", (norm_true/norm_c))\n",
    "print(\"Number Of dis =\", dis_c)\n",
    "print(\"\\nModel Accuracy dis=\", (dis_true/dis_c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "torch.save(model, './my_mnist_model.pt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
